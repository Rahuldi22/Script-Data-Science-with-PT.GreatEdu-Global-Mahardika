{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPc/iH1HyJ50nim+Ng6CkPp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Connect G-Drive\n","Run below code if only want to connect to your Drive!"],"metadata":{"id":"4G7BNP_rmcsY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aU9TRLCclT9E"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","files_dir = \"/content/drive/MyDrive/...\"\n","os.chdir(files_dir)"],"metadata":{"id":"quaB9uW8mgLI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# I.1 General Exploration"],"metadata":{"id":"03n1dLMAodTS"}},{"cell_type":"code","source":["import os\n","os.getcwd()"],"metadata":{"id":"966fs4cJR1NA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load libraries\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","sns.set(rc={\"figure.figsize\": (20, 15)})\n","sns.set_style(\"whitegrid\")\n","\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.impute import SimpleImputer\n","\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","from scipy.stats import norm\n","from scipy import stats\n","from scipy.stats import chi2_contingency"],"metadata":{"id":"wkqjKZB4m2cH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Train set\n","df_train = pd.read_csv(\"assets/train.csv\")\n","print(f\"Train set shape:\\n{df_train.shape}\\n\")\n","\n","# Load Test set\n","df_test = pd.read_csv(\"assets/test.csv\")\n","print(f\"Test set shape:\\n{df_test.shape}\")"],"metadata":{"id":"wOLVToPlot_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# info of each of the variables in our train set\n","df_train.info()"],"metadata":{"id":"nuDKSaceo-Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list comprehension\n","myList = [1, 2, 3, 4]\n","[val**2 for val in myList]"],"metadata":{"id":"xYX253CIlUGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking if column headings are the same in both data set\n","dif_1 = [x for x in df_train.columns if x not in df_test.columns] #\n","print(f\"Columns present in df_train and absent in df_test: {dif_1}\\n\")\n","\n","dif_2 = [x for x in df_test.columns if x not in df_train.columns]\n","print(f\"Columns present in df_test set and absent in df_train: {dif_2}\")"],"metadata":{"id":"h20x8PrepTPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"In0y1rc1mORK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test.head()"],"metadata":{"id":"tiwnsJRnmPCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.Street.value_counts()"],"metadata":{"id":"yM_-Xsz-OQy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.Street.value_counts(normalize=True)"],"metadata":{"id":"fsjZQ6ieRkHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the 'Id' column from the train set\n","df_train.drop([\"Id\"], axis=1, inplace=True)\n","# df_train.drop(columns=[\"Id\"], inplace=True)\n","\n","# Save the list of 'Id' before dropping it from the test set\n","Id_test_list = df_test[\"Id\"].tolist()\n","df_test.drop([\"Id\"], axis=1, inplace=True)"],"metadata":{"id":"oNoo8qxCpaC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test.head()"],"metadata":{"id":"EHL3GtbRJ74e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"id":"JWPMf_emKAKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# I.2 Numerical Features"],"metadata":{"id":"jUpEdEqRpfTM"}},{"cell_type":"markdown","source":["## I.2.1 Explore and Clean Numerical features"],"metadata":{"id":"MWVdiEHmpl3o"}},{"cell_type":"code","source":["# Let's select the columns of the train set with numerical data\n","df_train_num = df_train.select_dtypes(exclude=[\"object\"])\n","df_train_num.head()"],"metadata":{"id":"W2yrch7DpdOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's drop quasi-constant features where 95% of the values are similar or constant\n","# sel : selector\n","sel = VarianceThreshold(threshold=0.05) # 0.05: drop column where 95% of the values are constant\n","\n","# fit: finds the features with constant variance\n","sel.fit(df_train_num.drop(columns=\"SalePrice\")) # all cols except 'SalePrice'"],"metadata":{"id":"5Kqhjh2wqQgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sel.get_support()"],"metadata":{"id":"VTbsZesztcGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sum([True, True])"],"metadata":{"id":"jEyauEbZpFOu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the number of features that are not constant\n","print(f\"Number of retained features: {sum(sel.get_support())}\")\n","print(f\"\\nNumber of quasi_constant features: {len(df_train_num.iloc[:, :-1].columns) - sum(sel.get_support())}\")"],"metadata":{"id":"SV4htaFTrvUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_num.iloc[:, :-1].columns[sel.get_support()]"],"metadata":{"id":"ZbMCkBzYrpT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get quasi_constant feature(s)\n","df_train_num.iloc[:, :-1].columns[~sel.get_support()]"],"metadata":{"id":"1srV_avtMmb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.KitchenAbvGr.value_counts(normalize=True)"],"metadata":{"id":"NV2bPbv_R4A7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Notes!\n","# Example Case\n","myArr = np.array([1, 2, 3, 4])\n","myFilter = np.array([True, True, True, False])\n","myArr[myFilter]"],"metadata":{"id":"Pat-e6LVuH9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["myArr[~myFilter]"],"metadata":{"id":"jCsABNG6M_yF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["quasi_constant_features_list = (\n","    df_train_num\n","    .iloc[:, :-1]\n","    .columns[~sel.get_support()]\n","    .tolist()\n",")\n","\n","print(f\"\\nQuasi-constant features to be dropped: {quasi_constant_features_list}\")"],"metadata":{"id":"Q6nxsadDsFQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train[\"KitchenAbvGr\"].value_counts(dropna=False).sort_index()"],"metadata":{"id":"hqeXau6GNkqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train[\"KitchenAbvGr\"].value_counts(normalize=True).sort_index()"],"metadata":{"id":"DtbZynXYqlNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's drop these columns from df_train_num\n","df_train_num.drop(\n","    quasi_constant_features_list,\n","    axis=1,\n","    inplace=True\n",")"],"metadata":{"id":"zD1-3Jjqpsev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the distribution of all the numerical data\n","fig_ = df_train_num.hist(\n","    figsize=(16, 20),\n","    bins=50,\n","    color=\"deepskyblue\",\n","    edgecolor=\"black\",\n","    xlabelsize=8,\n","    ylabelsize=8\n",")"],"metadata":{"id":"trrcRzC8r_dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap for all the remaining numerical data including the taget 'SalePrice'\n","# Define the heatmap parameters\n","pd.options.display.float_format = \"{:,.2f}\".format"],"metadata":{"id":"ByaRrQet1SnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define correlation matrix\n","corr_matrix = df_train_num.corr()\n","corr_matrix"],"metadata":{"id":"V_9uj53g1Sg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace correlation < |0.3| by 0 for a better visibility\n","low_corr_filter = (corr_matrix < 0.3) & (corr_matrix > -0.3)\n","corr_matrix[low_corr_filter] = 0\n","corr_matrix"],"metadata":{"id":"_RlK9Gut1SY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mask the upper part of the heatmap\n","mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n","mask"],"metadata":{"id":"ijSckt872LDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose the color map\n","cmap = \"viridis\"\n","\n","# plot the heatmap\n","sns.heatmap(\n","    data = corr_matrix,\n","    mask = mask, # cover, not showing them which masked True\n","    vmax = 1.0,\n","    vmin = -1.0,\n","    linewidths = 0.1,\n","    annot_kws = {\n","        \"size\": 9,\n","        \"color\": \"black\"\n","    },\n","    square = True,\n","    cmap = cmap,\n","    annot = True\n",");"],"metadata":{"id":"Lubkj9QZzViC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's select features where the correlation with 'SalePrice' is higher than |0.3|\n","# -1 because the latest row is SalePrice\n","df_num_corr = df_train_num.corr()[\"SalePrice\"][:-1]\n","\n","# Correlated features (r2 > 0.5)\n","high_corr_mask = abs(df_num_corr) >= 0.5\n","high_features_list = df_num_corr[high_corr_mask].sort_values(ascending=False)\n","print(f\"{len(high_features_list)} strongly correlated values with SalePrice:\\n{high_features_list}\\n\")\n","\n","# Correlated features (0.3 < r2 < 0.5)\n","low_corr_mask = (abs(df_num_corr) < 0.5) & (abs(df_num_corr) >= 0.3)\n","low_features_list = df_num_corr[low_corr_mask].sort_values(ascending=False)\n","print(f\"{len(low_features_list)} slightly correlated values with SalePrice:\\n{low_features_list}\")"],"metadata":{"id":"MqK56WNa8jIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Features with high correlation (higher than 0.5)\n","strong_features = df_num_corr[abs(df_num_corr) >= 0.5].index.tolist()\n","strong_features.append(\"SalePrice\")\n","\n","df_strong_features = df_train_num.loc[:, strong_features]\n","\n","plt.style.use(\"seaborn-whitegrid\")  # define figures style\n","fig, ax = plt.subplots(round(len(strong_features) / 3), 3)\n","\n","for i, ax in enumerate(fig.axes):\n","    # plot the correlation of each feature with SalePrice\n","    if i < len(strong_features)-1:\n","        sns.regplot(\n","            x=strong_features[i],\n","            y=\"SalePrice\",\n","            data=df_strong_features,\n","            ax=ax,\n","            scatter_kws= {\"color\": \"deepskyblue\"},\n","            line_kws={\"color\": \"black\"}\n","        )"],"metadata":{"id":"zbok24tT_wH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_strong_features"],"metadata":{"id":"VOdC_vSJVwVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Features with low correlation (between 0.3 and 0.5)\n","low_features = df_num_corr[(abs(df_num_corr) >= 0.3) & (abs(df_num_corr) < 0.5)].index.tolist()\n","low_features.append(\"SalePrice\")\n","\n","df_low_features = df_train_num.loc[:, low_features]\n","\n","plt.style.use(\"seaborn-whitegrid\")  # define figures style\n","fig, ax = plt.subplots(round(len(low_features) / 3), 3)\n","\n","for i, ax in enumerate(fig.axes):\n","    # plot the correlation of each feature with SalePrice\n","    if i < len(low_features) - 1:\n","        sns.regplot(\n","            x=low_features[i],\n","            y=\"SalePrice\",\n","            data=df_low_features,\n","            ax=ax,\n","            scatter_kws={\"color\": \"deepskyblue\"},\n","            line_kws={\"color\": \"black\"}\n","        )"],"metadata":{"id":"qe-ROlufBLBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the list of numerical fetaures to keep\n","list_of_numerical_features = strong_features[:-1] + low_features\n","\n","# Let's select these features form our train set\n","df_train_num = df_train_num.loc[:, list_of_numerical_features]\n","\n","# The same features are selected from the test set (-1 -> except 'SalePrice')\n","df_test_num = df_test.loc[:, list_of_numerical_features[:-1]]"],"metadata":{"id":"tUXKqN8FBdAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_of_numerical_features"],"metadata":{"id":"QbaJ5BelBurb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## I.2.2 Missing data of Numerical features"],"metadata":{"id":"sddhA_BBB69L"}},{"cell_type":"markdown","source":["Train Set"],"metadata":{"id":"NFayqOu0CB1Y"}},{"cell_type":"code","source":["# Check the NaN of the train set by ploting percent of missing values per column\n","column_with_nan = df_train_num.columns[df_train_num.isnull().any()]\n","column_name = []\n","percent_nan = []\n","\n","for i in column_with_nan:\n","    column_name.append(i)\n","    percent_nan.append(round(df_train_num[i].isnull().sum()*100/len(df_train_num), 2))\n","\n","tab = pd.DataFrame(column_name, columns=[\"Column\"])\n","tab[\"Percent_NaN\"] = percent_nan\n","tab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n","\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (10, 7)})\n","sns.set_style(\"whitegrid\")\n","\n","# Plot results\n","p = sns.barplot(\n","    x=\"Percent_NaN\",\n","    y=\"Column\",\n","    data=tab,\n","    edgecolor=\"black\",\n","    color=\"deepskyblue\"\n",")\n","\n","p.set_title(\"Percent of NaN per column of the train set\\n\", fontsize=20)\n","p.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\n","p.set_ylabel(\"Column Name\\n\", fontsize=20);"],"metadata":{"id":"Jih2IVOHByOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imputation of missing values (NaNs) with SimpleImputer\n","my_imputer = SimpleImputer(strategy=\"median\")\n","df_train_imputed = pd.DataFrame(my_imputer.fit_transform(df_train_num))\n","df_train_imputed.head()"],"metadata":{"id":"uaPZ4GJPCWoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_imputed.columns = df_train_num.columns\n","df_train_imputed.head()"],"metadata":{"id":"bgluZKy0DERn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the distribution of each imputed feature before and after imputation\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (14, 12)})\n","sns.set_style(\"whitegrid\")\n","fig, axes = plt.subplots(3, 2)\n","\n","# Plot the results\n","for feature, fig_pos in zip([\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"], [0, 1, 2]):\n","\n","    \"\"\"Features distribution before and after imputation\"\"\"\n","\n","    # before imputation\n","    p = sns.histplot(ax=axes[fig_pos, 0], x=df_train_num[feature],\n","                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n","    p.set_ylabel(f\"Before imputation\", fontsize=14)\n","\n","    # after imputation\n","    q = sns.histplot(ax=axes[fig_pos, 1], x=df_train_imputed[feature],\n","                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\")\n","    q.set_ylabel(f\"After imputation\", fontsize=14)"],"metadata":{"id":"Yq8PR3lQDEOu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop 'LotFrontage' and 'GarageYrBlt'\n","# Karena jika diimputasi, akan ada perubahan drastis dari sisi distribusi data\n","# Mencegah model membuat prediksi yang lebih bias akibat imputasi missing value\n","df_train_imputed.drop(\n","    [\"LotFrontage\", \"GarageYrBlt\"],\n","    axis=1,\n","    inplace=True\n",")\n","df_train_imputed.head()"],"metadata":{"id":"na0VmL2_DELS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Set"],"metadata":{"id":"TgRB9Md5Dv1l"}},{"cell_type":"code","source":["# Drop the same features from test set as for the train set\n","df_test_num.drop(\n","    [\"LotFrontage\", \"GarageYrBlt\"],\n","    axis=1,\n","    inplace=True\n",")"],"metadata":{"id":"DglYiYlUDEIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the NaN of the test set by ploting percent of missing values per column\n","column_with_nan = df_test_num.columns[df_test_num.isnull().any()]\n","column_name = []\n","percent_nan = []\n","\n","for i in column_with_nan:\n","    column_name.append(i)\n","    percent_nan.append(\n","        round(df_test_num[i].isnull().sum()*100/len(df_test_num), 2))\n","\n","tab = pd.DataFrame(column_name, columns=[\"Column\"])\n","tab[\"Percent_NaN\"] = percent_nan\n","tab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n","\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (10, 7)})\n","sns.set_style(\"whitegrid\")\n","\n","# Plot results\n","p = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n","                edgecolor=\"black\", color=\"deepskyblue\")\n","\n","p.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\n","p.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\n","p.set_ylabel(\"Column Name\\n\", fontsize=20)"],"metadata":{"id":"hAzyKmObDEEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imputation of missing values (NaNs) with SimpleImputer\n","my_imputer = SimpleImputer(strategy=\"median\")\n","df_test_imputed = pd.DataFrame(my_imputer.fit_transform(df_test_num))\n","df_test_imputed.columns = df_test_num.columns"],"metadata":{"id":"MqvtBqyUDEBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the distribution of each imputed feature before and after imputation\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (20, 18)})\n","sns.set_style(\"whitegrid\")\n","fig, axes = plt.subplots(5, 2)\n","\n","# Plot the results\n","for feature, fig_pos in zip(tab[\"Column\"].tolist(), range(0, 6)):\n","\n","    \"\"\"Features distribution before and after imputation\"\"\"\n","\n","    # before imputation\n","    p = sns.histplot(ax=axes[fig_pos, 0], x=df_test_num[feature],\n","                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n","    p.set_ylabel(f\"Before imputation\", fontsize=14)\n","\n","    # after imputation\n","    q = sns.histplot(ax=axes[fig_pos, 1], x=df_test_imputed[feature],\n","                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\",)\n","    q.set_ylabel(f\"After imputation\", fontsize=14)\n","\n","fig.tight_layout()"],"metadata":{"id":"fA3vBErIDD93"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hasil imputasi tidak merubah secara signifikan dari data, akibat proporsi missing value yang kecil"],"metadata":{"id":"X1iLiZJ9aOr9"}},{"cell_type":"markdown","source":["# I.3 Categorical features"],"metadata":{"id":"M3e-uBjrEvgC"}},{"cell_type":"markdown","source":["## I.3.1. Explore and Clean Categorical features"],"metadata":{"id":"1BfqjhSXE4MI"}},{"cell_type":"code","source":["# Categorical to Quantitative relationship\n","\n","# categorical_features = [i for i in df_train.columns if df_train.dtypes[i] == \"object\"] # -->list comprehension\n","\n","# for-loop\n","categorical_features = []\n","for feat, feat_type in zip(df_train.columns, df_train.dtypes):\n","    if feat_type == 'object':\n","       categorical_features.append(feat)\n","\n","# Include target variable\n","categorical_features.append(\"SalePrice\")\n","\n","# Train set\n","df_train_categ = df_train[categorical_features]\n","\n","# Test set (-1 because test set don't have 'Sale Price')\n","df_test_categ = df_test[categorical_features[:-1]]\n","\n","df_test_categ.head()"],"metadata":{"id":"rFBKoUKYEvF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categorical_features"],"metadata":{"id":"dc6CL8j-b5sq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_categ['MSZoning']"],"metadata":{"id":"bGjSr7ficFL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"0cJQLsXsfJTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Countplot for each of the categorical features in the train set\n","fig, axes = plt.subplots(\n","    round(len(df_train_categ.columns)/3), 3, figsize=(12, 30))\n","\n","for i, ax in enumerate(fig.axes):\n","    # plot barplot of each feature\n","    if i < len(df_train_categ.columns) - 1:\n","        sns.countplot(\n","            x=df_train_categ.columns[i], alpha=0.7,\n","            data=df_train_categ, ax=ax, legend=False,\n","            hue=df_train_categ.columns[i])\n","        ax.set_xticklabels(\n","            ax.xaxis.get_majorticklabels(), rotation=45)\n","\n","fig.tight_layout()\n","fig.delaxes(axes[-1, -1])"],"metadata":{"id":"UOfa1tfnDD5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop some categorical 'non-informative' features from train set\n","columns_to_drop = [\n","    'Street', 'Alley', 'LandContour',\n","    'Utilities', 'LandSlope', 'Condition2',\n","    'RoofMatl', 'CentralAir', 'BsmtFinType2',\n","    'Heating', 'Functional', 'GarageQual',\n","    'GarageCond','ExterCond','MiscFeature',\n","    'PavedDrive', 'SaleType'\n","]"],"metadata":{"id":"Sz2OdVJQDD1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train set\n","df_train_categ.drop(columns_to_drop, axis=1, inplace=True)\n","\n","# Test set\n","df_test_categ.drop(columns_to_drop, axis=1, inplace=True)"],"metadata":{"id":"s_QHAk9ODDtl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# With the boxplot we can see the variation of the target 'SalePrice' in each of the categorical features\n","fig, axes = plt.subplots(\n","    round(len(df_train_categ.columns)/3), 3, figsize=(15, 30))\n","\n","for i, ax in enumerate(fig.axes):\n","    # plot the variation of SalePrice in each feature\n","    if i < len(df_train_categ.columns) - 1:\n","        sns.boxplot(\n","            x=df_train_categ.columns[i], y=\"SalePrice\",\n","            data=df_train_categ, legend=False, ax=ax,\n","            hue=df_train_categ.columns[i])\n","        ax.set_xticklabels(\n","            ax.xaxis.get_majorticklabels(), rotation=45)\n","\n","fig.tight_layout()\n","fig.delaxes(axes[-1, -1])"],"metadata":{"id":"hGmSBFNBDDof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_categ[\"Exterior1st\"].value_counts()"],"metadata":{"id":"Cb30zMoZzZ7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_categ[\"Exterior2nd\"].value_counts()"],"metadata":{"id":"nUaJ8RJYzh2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot contingency table\n","\n","sns.set(rc={\"figure.figsize\": (10, 7)})\n","\n","X = [\"Exterior1st\", \"ExterQual\", \"BsmtQual\", \"BsmtQual\"]\n","Y = [\"Exterior2nd\", \"MasVnrType\", \"BsmtCond\", \"BsmtExposure\"]\n","\n","for i, j in zip(X, Y):\n","\n","    # Contingency table\n","    cont = df_train_categ[[i, j]].pivot_table(\n","        index=i,\n","        columns=j,\n","        aggfunc=len,\n","        margins=True,\n","        margins_name=\"Total\"\n","    )\n","\n","    tx = cont.loc[:, [\"Total\"]]\n","    ty = cont.loc[[\"Total\"], :]\n","    n = len(df_train_categ)\n","    indep = tx.dot(ty) / n\n","    c = cont.fillna(0)  # Replace NaN with 0 in the contingency table\n","    measure = (c - indep) ** 2 / indep\n","    xi_n = measure.sum().sum()\n","    table = measure / xi_n\n","\n","    # Plot contingency table\n","    p = sns.heatmap(table.iloc[:-1, :-1],\n","                    annot=c.iloc[:-1, :-1], fmt=\".0f\", cmap=\"Oranges\")\n","    p.set_xlabel(j, fontsize=18)\n","    p.set_ylabel(i, fontsize=18)\n","    p.set_title(f\"\\nχ² test between groups {i} and groups {j}\\n\", size=18)\n","    plt.show()\n","\n","    # Performing Chi-sq test\n","    CrosstabResult = pd.crosstab(\n","        index=df_train_categ[i], columns=df_train_categ[j])\n","    ChiSqResult = chi2_contingency(CrosstabResult)\n","\n","    # P-Value is the Probability of H0 being True\n","    print(f\"P-Value of the ChiSq Test bewteen {i} and {j} is: {ChiSqResult[1]}\\n\")"],"metadata":{"id":"ujAV92pVOCq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dalam analisis uji chi-square, indikasi bahwa dua atau lebih variabel kategorikal memiliki hubungan erat adalah ketika nilai-nilai yang diamati dalam contingency table berbeda secara signifikan dari nilai-nilai yang diharapkan jika tidak ada hubungan antara variabel-variabel tersebut.\n","\n","- Hipotesis Awal (Null Hypothesis, H0):\n","Hipotesis awal adalah bahwa **tidak ada hubungan** antara dua atau lebih variabel kategorikal. Dinyatakan sebagai \"Tidak ada perbedaan antara variabel-variabel yang diamati dan yang diharapkan jika tidak ada hubungan antara mereka.\"\n","\n","- Hipotesis Alternatif (Alternative Hypothesis, Ha):\n","Hipotesis alternatif menyatakan bahwa **ada hubungan** antara dua atau lebih variabel kategorikal. Dinyatakan sebagai \"Ada perbedaan antara variabel-variabel yang diamati dan yang diharapkan jika tidak ada hubungan antara mereka.\"\n","\n","\n","Jika p-value lebih kecil dari tingkat signifikansi yang ditetapkan, maka kita akan menolak hipotesis nol dan menerima hipotesis alternatif. Namun, jika p-value lebih besar dari tingkat signifikansi, kita gagal menolak hipotesis nol."],"metadata":{"id":"5-KsfLiEkn1p"}},{"cell_type":"code","source":["# Let's drop the one of each co-dependent variables\n","# Train set\n","df_train_categ.drop(Y, axis=1, inplace=True)\n","\n","# Test set\n","df_test_categ.drop(Y, axis=1, inplace=True)"],"metadata":{"id":"WKqoPxqAOUqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## I.3.2 Missing data of Categorical features"],"metadata":{"id":"c0e-HP4zQn2t"}},{"cell_type":"markdown","source":["Train Set"],"metadata":{"id":"iog3ObuNQ2lm"}},{"cell_type":"code","source":["# Check the NaN of the test set by ploting percent of missing values per column\n","column_with_nan = df_train_categ.columns[df_train_categ.isnull().any()]\n","column_name = []\n","percent_nan = []\n","\n","for i in column_with_nan:\n","    column_name.append(i)\n","    percent_nan.append(\n","        round(df_train_categ[i].isnull().sum() * 100 / len(df_train_categ), 2))\n","\n","tab = pd.DataFrame(column_name, columns=[\"Column\"])\n","tab[\"Percent_NaN\"] = percent_nan\n","tab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n","\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (10, 7)})\n","sns.set_style(\"whitegrid\")\n","\n","# Plot results\n","p = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n","                edgecolor=\"black\", color=\"deepskyblue\")\n","p.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\n","p.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\n","p.set_ylabel(\"Column Name\\n\", fontsize=20)"],"metadata":{"id":"nNBwA3uiQkZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tab[tab.Percent_NaN > 5]"],"metadata":{"id":"x7qzuBrg0906"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the features where the percentage of NaN is higher than 5%\n","# Why? Imputasi akan merubah distribusi secara signifikan\n","df_train_categ.drop(\n","    [\"PoolQC\", \"Fence\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\"],\n","    axis=1,\n","    inplace=True\n",")"],"metadata":{"id":"XxYwoEkiQ689"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill the NaN of each feature by the corresponding modal class\n","categ_fill_null = {\n","    \"BsmtQual\": df_train_categ[\"BsmtQual\"].mode().iloc[0],\n","    \"BsmtFinType1\": df_train_categ[\"BsmtFinType1\"].mode().iloc[0],\n","    \"Electrical\": df_train_categ[\"Electrical\"].mode().iloc[0]\n","}\n","\n","df_train_categ = df_train_categ.fillna(value=categ_fill_null)"],"metadata":{"id":"2etV_2bGc3qx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Set"],"metadata":{"id":"Xee0HRfhcXEa"}},{"cell_type":"code","source":["# Drop the same features from test set as for the train set\n","df_test_categ.drop(\n","    [\"PoolQC\", \"Fence\", \"FireplaceQu\",\"GarageType\", \"GarageFinish\"],\n","    axis=1,\n","    inplace=True\n",")"],"metadata":{"id":"gL-ObDo2cWrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the NaN of the test set by ploting percent of missing values per column\n","column_with_nan = df_test_categ.columns[df_test_categ.isnull().any()]\n","column_name = []\n","percent_nan = []\n","\n","for i in column_with_nan:\n","    column_name.append(i)\n","    percent_nan.append(\n","        round(df_test_categ[i].isnull().sum() * 100 / len(df_test_categ), 2))\n","\n","tab = pd.DataFrame(column_name, columns=[\"Column\"])\n","tab[\"Percent_NaN\"] = percent_nan\n","tab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n","\n","\n","# Define figure parameters\n","sns.set(rc={\"figure.figsize\": (10, 7)})\n","sns.set_style(\"whitegrid\")\n","\n","# Plot results\n","p = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n","                edgecolor=\"black\", color=\"deepskyblue\")\n","p.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\n","p.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\n","p.set_ylabel(\"Column Name\\n\", fontsize=20)"],"metadata":{"id":"vISb2TeibhYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill the NaN of each feature by the corresponding modal class\n","categ_fill_null = {\n","    \"BsmtQual\": df_train_categ[\"BsmtQual\"].mode().iloc[0],\n","    \"BsmtFinType1\": df_train_categ[\"BsmtFinType1\"].mode().iloc[0],\n","    \"MSZoning\": df_train_categ[\"MSZoning\"].mode().iloc[0],\n","    \"Exterior1st\": df_train_categ[\"Exterior1st\"].mode().iloc[0],\n","    \"KitchenQual\": df_train_categ[\"KitchenQual\"].mode().iloc[0]\n","}\n","\n","df_test_categ = df_test_categ.fillna(value=categ_fill_null)"],"metadata":{"id":"SMiEzdr9bhOZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## I.3.3. Transform Categorical features into One-Hot Encoded features (get_dummies)"],"metadata":{"id":"V-T78VGSeQGp"}},{"cell_type":"code","source":["# Train set\n","for i in df_train_categ.columns.tolist()[:-1]:\n","    df_dummies = pd.get_dummies(df_train_categ[i], prefix=i)\n","\n","    # merge both tables\n","    df_train_categ = df_train_categ.join(df_dummies)\n","\n","# Select the binary features only\n","df_train_binary = df_train_categ.iloc[:, 18:]\n","df_train_binary.head()"],"metadata":{"id":"yGNvXU28dcwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test set\n","for i in df_test_categ.columns.tolist():\n","    df_dummies = pd.get_dummies(df_test_categ[i], prefix=i)\n","\n","    # merge both tables\n","    df_test_categ = df_test_categ.join(df_dummies)\n","\n","# Select the binary features only\n","df_test_binary = df_test_categ.iloc[:, 17:]\n","df_test_binary.head()"],"metadata":{"id":"b3RA4zmGeT8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check if the column headings are the same in both data set, df_train and df_test\n","dif_1 = [x for x in df_train_binary.columns if x not in df_test_binary.columns]\n","print(f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n","\n","dif_2 = [x for x in df_test_binary.columns if x not in df_train_binary.columns]\n","print(f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")"],"metadata":{"id":"002N9FxNedbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's drop these columns from df_train_binary\n","df_train_binary.drop(dif_1, axis=1, inplace=True)\n","\n","# Check again if the column headings are the same in both data set\n","dif_1 = [x for x in df_train_binary.columns if x not in df_test_binary.columns]\n","print(f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n","\n","dif_2 = [x for x in df_test_binary.columns if x not in df_train_binary.columns]\n","print(f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")"],"metadata":{"id":"bpxwluaLervh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# I.4. Merge numerical and binary features into one data set"],"metadata":{"id":"OeHynBCKf4Ki"}},{"cell_type":"code","source":["# Add binary features to numreical features\n","# Train set\n","df_train_new = df_train_imputed.join(df_train_binary)\n","print(f\"Train set: {df_train_new.shape}\")\n","\n","# Test set\n","df_test_new = df_test_imputed.join(df_test_binary)\n","print(f\"Test set: {df_test_new.shape}\")"],"metadata":{"id":"ajvkfIUafwZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## I.5. Drop outliers from the train set"],"metadata":{"id":"wD4XbFLZkENP"}},{"cell_type":"code","source":["sns.violinplot(data=df_train, y=\"OpenPorchSF\")"],"metadata":{"id":"ruGRTP0tgfTN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop \"WoodDeckSF\" and \"OpenPorchSF\" --> See Numerical Features Viz\n","df_train_new.drop([\"WoodDeckSF\", \"OpenPorchSF\"], axis=1, inplace=True)\n","df_test_new.drop([\"WoodDeckSF\", \"OpenPorchSF\"], axis=1, inplace=True)"],"metadata":{"id":"MCNv-bvRf7Jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_new[(df_train_new[\"GrLivArea\"] > 4000) & (df_train_new[\"SalePrice\"] <= 200000)]"],"metadata":{"id":"FfIzsOQQ3WL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding Outliers\n","\n","# Create a 2x2 grid of subplots\n","fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n","\n","# Flatten the axs array for easier iteration\n","axs = axs.flatten()\n","\n","# Features to plot\n","features = [\"GrLivArea\", \"TotalBsmtSF\", \"GarageArea\"]\n","\n","# Iterate over each feature and plot against SalePrice\n","for i, feature in enumerate(features):\n","    ax = axs[i]\n","    ax.scatter(df_train_new[feature], df_train_new['SalePrice'], s=10)\n","    ax.set_title(f'{feature} vs SalePrice')\n","    ax.set_xlabel(feature)\n","    ax.set_ylabel('SalePrice')\n","\n","# Hide the last subplot\n","axs[-1].axis('off')\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"wAXQH5QapYEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a boxplot using Seaborn\n","sns.boxplot(x='SalePrice', data=df_train_new);"],"metadata":{"id":"9VHi9XIGrnF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's handle the outliers in \"GrLivArea\", \"TotalBsmtSF\" and \"GarageArea\"\n","\n","# Outliers in \"GrLivArea\"\n","outliers1 = df_train_new[(df_train_new[\"GrLivArea\"] > 4000) & (\n","    df_train_new[\"SalePrice\"] <= 200000)].index.tolist()\n","\n","# Outliers in \"TotalBsmtSF\"\n","outliers2 = df_train_new[(df_train_new[\"TotalBsmtSF\"] > 3000) & (\n","    df_train_new[\"SalePrice\"] <= 400000)].index.tolist()\n","\n","# Outliers in \"GarageArea\"\n","outliers3 = df_train_new[(df_train_new[\"GarageArea\"] > 1200) & (\n","    df_train_new[\"SalePrice\"] <= 300000)].index.tolist()\n","\n","# List of all the outliers\n","outliers = outliers1 + outliers2 + outliers3\n","outliers = list(set(outliers))\n","print(outliers)\n","\n","# Drop these outlier\n","df_train_new = df_train_new.drop(df_train_new.index[outliers])\n","\n","# Reset index\n","df_train_new = df_train_new.reset_index().drop(\"index\", axis=1)"],"metadata":{"id":"v_sVpy1hgfR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# II. Feature engineering"],"metadata":{"id":"NR6YJPp7g6WB"}},{"cell_type":"code","source":["# Define a function to calculate the occupancy rate of the first floor of the total living area\n","\n","\n","def floor_occupation(x):\n","    \"\"\"First floor occupation of the total live area\n","\n","    floor_occupation equation has the following form:\n","    (1st Floor Area * 100) / (Ground Live Area)\n","\n","    Args:\n","        x -- the corresponding feature\n","\n","    Returns:\n","        0 -- if Ground Live Area = 0\n","        equation -- if Ground Live Area > 0\n","    \"\"\"\n","    if x[\"GrLivArea\"] == 0:\n","        return 0\n","    else:\n","        return x[\"1stFlrSF\"] * 100 / x[\"GrLivArea\"]\n","\n","\n","# Apply the function on train and test set\n","df_train_new[\"1stFlrPercent\"] = df_train_new.apply(\n","    lambda x: floor_occupation(x), axis=1)\n","\n","df_test_new[\"1stFlrPercent\"] = df_test_new.apply(\n","    lambda x: floor_occupation(x), axis=1)\n","\n","# Drop \"1stFlrSF\" and \"2ndFlrSF\"\n","df_train_new.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)\n","df_test_new.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)"],"metadata":{"id":"TJeS9sDLgiOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to calculate the occupancy rate of the finished basement area\n","\n","\n","def bsmt_finish(x):\n","    \"\"\"Propotion of finished area in basement\n","\n","    bsmt_finish equation has the following form:\n","    (Finished Basement Area * 100) / (Total Basement Area)\n","\n","    Args:\n","        x -- the corresponding feature\n","\n","    Returns:\n","        0 -- if Total Basement Area = 0\n","        equation -- if Total Basement Area > 0\n","    \"\"\"\n","    if x[\"TotalBsmtSF\"] == 0:\n","        return 0\n","    else:\n","        return x[\"BsmtFinSF1\"] * 100 / x[\"TotalBsmtSF\"]\n","\n","\n","# Apply the function on train and test set\n","df_train_new[\"BsmtFinPercent\"] = df_train_new.apply(\n","    lambda x: bsmt_finish(x), axis=1)\n","\n","df_test_new[\"BsmtFinPercent\"] = df_test_new.apply(\n","    lambda x: bsmt_finish(x), axis=1)\n","\n","# Drop \"BsmtFinSF1\"\n","df_train_new.drop([\"BsmtFinSF1\"], axis=1, inplace=True)\n","df_test_new.drop([\"BsmtFinSF1\"], axis=1, inplace=True)"],"metadata":{"id":"81xD0qUCg6E9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert Year of construction to Age of the house since the construction\n","df_train_new[\"AgeSinceConst\"] = df_train_new[\"YearBuilt\"].max() - df_train_new[\"YearBuilt\"]\n","\n","df_test_new[\"AgeSinceConst\"] = df_train_new[\"YearBuilt\"].max() - df_test_new[\"YearBuilt\"]\n","\n","# Drop \"YearBuilt\"\n","df_train_new.drop([\"YearBuilt\"], axis=1, inplace=True)\n","df_test_new.drop([\"YearBuilt\"], axis=1, inplace=True)"],"metadata":{"id":"OIJr726Fg6CP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_new[\"AgeSinceRemod\"] = df_train_new[\"YearRemodAdd\"].max() - df_train_new[\"YearRemodAdd\"]\n","\n","df_test_new[\"AgeSinceRemod\"] = df_train_new[\"YearRemodAdd\"].max() - df_test_new[\"YearRemodAdd\"]\n","\n","# Drop \"YearRemodAdd\"\n","df_train_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)\n","df_test_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)"],"metadata":{"id":"QFribe-8iW1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See Numerical Feature Viz\n","continuous_features = [\n","    \"OverallQual\", \"TotalBsmtSF\", \"GrLivArea\",\n","    \"FullBath\", \"TotRmsAbvGrd\", \"GarageCars\", \"GarageArea\",\n","    \"MasVnrArea\", \"Fireplaces\", \"1stFlrPercent\",\n","    \"BsmtFinPercent\", \"AgeSinceConst\", \"AgeSinceRemod\"\n","]\n","\n","df_skew_verify = df_train_new.loc[:, continuous_features]"],"metadata":{"id":"-UXhrOzjg5-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select features with absolute Skew higher than 0.5\n","skew_ft = []\n","\n","for i in continuous_features:\n","    # list of skew for each corresponding feature\n","    skew_ft.append(abs(df_skew_verify[i].skew()))\n","\n","df_skewed = pd.DataFrame(\n","    {\n","        \"Columns\": continuous_features,\n","        \"Abs_Skew\": skew_ft\n","    }\n",")\n","\n","sk_features = df_skewed[df_skewed[\"Abs_Skew\"] > 0.5][\"Columns\"].tolist()\n","print(f\"List of skewed features: {sk_features}\")"],"metadata":{"id":"rcMm4DpKg5qq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Log transformation of the skewed features\n","# sf_features = [\"TotalBsmtSF\", \"GrLivArea\", \"MasVnrArea\", \"GarageArea\"]\n","\n","for i in sk_features:\n","    # loop over i (features) to calculate Log of surfaces\n","    # Train set\n","    df_train_new[i] = np.log((df_train_new[i])+1)\n","\n","    # Test set\n","    df_test_new[i] = np.log((df_test_new[i])+1)"],"metadata":{"id":"Dz_m-uCWg5gq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# III. Preparing data for modeling"],"metadata":{"id":"RmiTLK5UjCKc"}},{"cell_type":"markdown","source":["## III.1 Target variable 'SalePrice'"],"metadata":{"id":"8p8GSuAfjMcW"}},{"cell_type":"code","source":["# Log transformation of the target variable \"SalePrice\"\n","df_train_new[\"SalePriceLog\"] = np.log(df_train_new.SalePrice)\n","\n","# Plot the distribution before and after transformation\n","fig, axes = plt.subplots(1, 2)\n","fig.suptitle(\"Distribution of 'SalePrice' before and after log-transformation\")\n","\n","# before log transformation\n","p = sns.histplot(ax=axes[0], x=df_train_new[\"SalePrice\"],\n","                 kde=True, bins=100, color=\"deepskyblue\")\n","p.set_xlabel(\"SalePrice\", fontsize=16)\n","p.set_ylabel(\"Effective\", fontsize=16)\n","\n","# after log transformation\n","q = sns.histplot(ax=axes[1], x=df_train_new[\"SalePriceLog\"],\n","                 kde=True, bins=100, color=\"darkorange\")\n","q.set_xlabel(\"SalePriceLog\", fontsize=16)\n","q.set_ylabel(\"\", fontsize=16);"],"metadata":{"id":"DALachckjAK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the original SalePrice\n","df_train_new.drop([\"SalePrice\"], axis=1, inplace=True)"],"metadata":{"id":"Yz9K0hjHjTBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## III.2. Split data into train and test set and Standardization"],"metadata":{"id":"JBWZM-19jZVA"}},{"cell_type":"code","source":["# Extract the features (X) and the target (y)\n","# Features (X)\n","X = df_train_new[[i for i in list(df_train_new.columns) if i != \"SalePriceLog\"]]\n","# X = df_train_new.drop(columns=[\"SalePriceLog\"])\n","print(X.shape)\n","\n","# Target (y)\n","y = df_train_new.loc[:, \"SalePriceLog\"]\n","print(y.shape)"],"metadata":{"id":"Rq7oV0rWjWTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualization on 'SalePriceLog'\n","\n","fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n","\n","# Plot Empiric Cummulative Distribution\n","sorted_data = y.sort_values(ignore_index=True)\n","axs[0].scatter(x=sorted_data.index, y=sorted_data)\n","axs[0].set_title('Empiric Cummulative Distribution of SalePriceLog')\n","\n","# Plot histogram of 'SalePrice'\n","axs[1].hist(y, bins=30, color='blue', alpha=0.7)\n","axs[1].set_title('Histogram for SalePriceLog')"],"metadata":{"id":"UnZWUasuwFhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split into X_train and X_test (by stratifying on y)\n","# Stratify on a continuous variable by splitting it in bins\n","# Create the bins.\n","bins = np.linspace(min(y) + 0.5, max(y) - 0.5, 10)\n","y_binned = np.digitize(y, bins)\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, stratify=y_binned, shuffle=True)\n","\n","print(f\"X_train:{X_train.shape}\\ny_train:{y_train.shape}\")\n","print(f\"\\nX_test:{X_test.shape}\\ny_test:{y_test.shape}\")"],"metadata":{"id":"po7adWSAjboN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the data\n","std_scale = preprocessing.StandardScaler().fit(X_train)\n","X_train = std_scale.transform(X_train)\n","X_test = std_scale.transform(X_test)\n","\n","# The same standardization is applied for df_test_new\n","df_test_new = std_scale.transform(df_test_new)\n","\n","# The output of standardization is a vector. Let's turn it into a table\n","# Convert X, y and test data into dataframe\n","X_train = pd.DataFrame(X_train, columns=X.columns)\n","X_test = pd.DataFrame(X_test, columns=X.columns)\n","df_test_new = pd.DataFrame(df_test_new, columns=X.columns)\n","\n","y_train = pd.DataFrame(y_train)\n","y_train = y_train.reset_index().drop(\"index\", axis=1)\n","\n","y_test = pd.DataFrame(y_test)\n","y_test = y_test.reset_index().drop(\"index\", axis=1)"],"metadata":{"id":"lfugOHQRjeYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## III.3. Backward Stepwise Regression\n","Source: [Link](https://www.analystsoft.com/en/products/statplus/content/help/pdf/analysis_regression_backward_stepwise_elimination_regression_model.pdf)"],"metadata":{"id":"RfXXZmVjjwtH"}},{"cell_type":"markdown","source":["Backward Stepwise Regression adalah metode dalam analisis regresi yang digunakan untuk membangun model regresi dengan cara memulai dengan model yang sudah terdapat semua prediktor, kemudian secara bertahap menghapus prediktor-prediktor yang dianggap tidak signifikan atau kurang relevan satu per satu dari model, hingga hanya tersisa prediktor-prediktor yang dianggap signifikan.\n","\n","Prosesnya dimulai dengan membangun model regresi dengan menggunakan semua prediktor yang tersedia. Kemudian, langkah demi langkah, prediktor yang memiliki koefisien regresi yang tidak signifikan (biasanya ditentukan berdasarkan nilai p-value yang melebihi ambang batas yang telah ditetapkan, misalnya 0.05) akan dihapus satu per satu dari model. Setiap kali prediktor dihapus, model akan dianalisis ulang untuk memastikan bahwa penghapusan prediktor tersebut tidak mempengaruhi signifikansi prediktor lainnya dalam model.\n","\n","Tujuan dari Backward Stepwise Regression adalah untuk menyederhanakan model regresi sehingga hanya menyertakan prediktor-prediktor yang benar-benar penting atau signifikan dalam menjelaskan variabilitas variabel respons, sehingga memudahkan interpretasi model dan mengurangi risiko overfitting. Metode ini sangat berguna terutama ketika jumlah prediktor dalam model sangat besar dan kita ingin memilih hanya prediktor-prediktor yang paling berpengaruh."],"metadata":{"id":"2IS6fO225LNZ"}},{"cell_type":"markdown","source":["Algoritma Backward Stepwise Regression:\n","\n","Berikut adalah algoritma Backward Stepwise Regression secara umum:\n","\n","1. Bangun model regresi menggunakan semua prediktor yang tersedia.\n","2. Lakukan uji hipotesis untuk menentukan signifikansi masing-masing koefisien regresi (misalnya, menggunakan nilai p-value dengan ambang batas tertentu, seperti 0.05).\n","3. Identifikasi prediktor dengan koefisien regresi yang tidak signifikan (p-value melebihi ambang batas yang ditetapkan).\n","4. Hapus satu prediktor yang tidak signifikan tersebut dari model.\n","5. Rekonstruksi ulang model tanpa prediktor yang dihapus.\n","6. Ulangi langkah 2-5 hingga semua prediktor dalam model dianggap signifikan (semua p-value kurang dari ambang batas yang ditetapkan).\n","7. Selesaikan proses ketika tidak ada lagi prediktor yang dapat dihapus dari model.\n","8. Evaluasi dan validasi model yang dihasilkan.\n"],"metadata":{"id":"3-Az5EFY59J8"}},{"cell_type":"code","source":["def backward_regression(\n","        X, y, initial_list=[],\n","        threshold_in=0.01, threshold_out=0.05, verbose=True):\n","    \"\"\"To select feature with Backward Stepwise Regression\n","\n","    Args:\n","        X -- features values\n","        y -- target variable\n","        initial_list -- features header\n","        threshold_in -- pvalue threshold of features to keep\n","        threshold_out -- pvalue threshold of features to drop\n","        verbose -- true to produce lots of logging output\n","\n","    Returns:\n","        list of selected features for modeling\n","    \"\"\"\n","    included = list(X.columns)\n","    while True:\n","        changed = False\n","        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n","        # use all coefs except intercept\n","        pvalues = model.pvalues.iloc[1:]\n","        worst_pval = pvalues.max()  # null if p-values is empty\n","        if worst_pval > threshold_out:\n","            changed = True\n","            worst_feature = pvalues.idxmax()\n","            included.remove(worst_feature)\n","            if verbose:\n","                print(f\"worst_feature : {worst_feature}, {worst_pval} \")\n","        if not changed:\n","            break\n","    print(f\"\\nSelected Features:\\n{included}\")\n","    return included\n","\n","# Application of the backward regression function on our training data\n","Selected_Features = backward_regression(X_train, y_train)"],"metadata":{"id":"xJJI287vjscG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Keep the selected features only\n","X_train = X_train.loc[:, Selected_Features]\n","X_test = X_test.loc[:, Selected_Features]\n","df_test_new = df_test_new.loc[:, Selected_Features]"],"metadata":{"id":"onNHfGMCkbu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## III.4. Variance Inflation Factor\n","Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable.\n","\n","A feature with a VIF higher than 10 implies (5 is also common) that there is a multi-collinearity with the latter.\n","\n","Source: [Link](https://www.investopedia.com/terms/v/variance-inflation-factor.asp#:~:text=Variance%20inflation%20factor%20(VIF)%20is,only%20that%20single%20independent%20variable)"],"metadata":{"id":"54wf5Gylk1Fu"}},{"cell_type":"code","source":["# Here I calculate VIF for each feature\n","vif = pd.DataFrame()\n","vif[\"VIF Factor\"] = [variance_inflation_factor(\n","    X_train.values, i) for i in range(X_train.shape[1])]\n","\n","# VIF results in a table\n","vif[\"features\"] = X_train.columns\n","vif.round(2)"],"metadata":{"id":"9nVnr87tkyWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select features with high VIF\n","high_vif_list = vif[vif[\"VIF Factor\"] > 10][\"features\"].tolist()\n","\n","if len(high_vif_list) == 0:\n","    # print empty list if low multicolinearity\n","    print(f\"None of the features have a high multicollinearity\")\n","else:\n","    # print list of features with high multicolinearity\n","    print(f\"List of features with high multicollinearity: {high_vif_list}\")"],"metadata":{"id":"NpMdNV-Ck8AR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The strong muticollinearity is probably due to the presence of a lot 0's in these binary features. These features must be dropped"],"metadata":{"id":"z2CxhMkW-ucu"}},{"cell_type":"code","source":["# Drop features with high multicollinearity from X_train, X_test and df_test_new\n","X_train.drop(high_vif_list, axis=1, inplace=True)\n","\n","X_test.drop(high_vif_list, axis=1, inplace=True)\n","\n","df_test_new.drop(high_vif_list, axis=1, inplace=True)"],"metadata":{"id":"Ia0gt8JLlJLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## III.5. Cook distance"],"metadata":{"id":"4GO4FTKtltrv"}},{"cell_type":"markdown","source":["By calculating Cook distance we can detect influential observations in a regression model. Cook distance detects data with large residuals (outliers) that can distort the prediction and the accuracy of a regression."],"metadata":{"id":"tKjp3mMb-4ax"}},{"cell_type":"code","source":["X_constant = sm.add_constant(X_train)\n","\n","model = sm.OLS(y_train, X_constant)\n","lr = model.fit()\n","\n","# Cook distance\n","np.set_printoptions(suppress=True)\n","\n","# Create an instance of influence\n","influence = lr.get_influence()\n","\n","# Get Cook's distance for each observation\n","cooks = influence.cooks_distance\n","\n","# Result as a dataframe\n","cook_df = pd.DataFrame({\"Cook_Distance\": cooks[0], \"p_value\": cooks[1]})\n","cook_df.head()"],"metadata":{"id":"182RJwJzlrft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cook_df.shape"],"metadata":{"id":"9M8r6vE99RqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove the influential observation from X_train and y_train\n","influent_observation = cook_df[cook_df[\"p_value\"] < 0.05].index.tolist()\n","print(f\"Influential observations dropped: {influent_observation}\")\n","\n","# Drop these obsrevations\n","X_train = X_train.drop(X_train.index[influent_observation])\n","y_train = y_train.drop(y_train.index[influent_observation])"],"metadata":{"id":"sKJBZ5KIlxXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["influent_observation"],"metadata":{"id":"q4oTZzpmhmcY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# IV. Modeling"],"metadata":{"id":"M6cJd14imJ5r"}},{"cell_type":"markdown","source":["## IV.1. Models and metrics selection"],"metadata":{"id":"1KKTWKc8mOZR"}},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.linear_model import Ridge, Lasso, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor"],"metadata":{"id":"xaMCJrflmN2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's define a function for each metrics\n","# R²\n","def rsqr_score(test, pred):\n","    \"\"\"Calculate R squared score\n","\n","    Args:\n","        test -- test data\n","        pred -- predicted data\n","\n","    Returns:\n","        R squared score\n","    \"\"\"\n","    r2_ = r2_score(test, pred)\n","    return r2_\n","\n","\n","# RMSE\n","def rmse_score(test, pred):\n","    \"\"\"Calculate Root Mean Square Error score\n","\n","    Args:\n","        test -- test data\n","        pred -- predicted data\n","\n","    Returns:\n","        Root Mean Square Error score\n","    \"\"\"\n","    rmse_ = np.sqrt(mean_squared_error(test, pred))\n","    return rmse_\n","\n","\n","# Print the scores\n","def print_score(test, pred):\n","    \"\"\"Print calculated score\n","\n","    Args:\n","        test -- test data\n","        pred -- predicted data\n","\n","    Returns:\n","        print the regressor name\n","        print the R squared score\n","        print Root Mean Square Error score\n","    \"\"\"\n","\n","    print(f\"- Regressor: {regr.__class__.__name__}\")\n","    print(f\"R²: {rsqr_score(test, pred)}\")\n","    print(f\"RMSE: {rmse_score(test, pred)}\\n\")"],"metadata":{"id":"U9E73M4vmJjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define regression models\n","dtr = DecisionTreeRegressor()\n","ridge = Ridge()\n","lasso = Lasso(alpha=0.001)\n","elastic = ElasticNet(alpha=0.001)\n","rdf = RandomForestRegressor()\n","xgboost = XGBRegressor()\n","lgbm = LGBMRegressor()\n","\n","# Train models on X_train and y_train\n","for regr in [dtr, ridge, lasso, elastic, rdf, xgboost, lgbm]:\n","    # fit the corresponding model\n","    regr.fit(X_train, y_train)\n","    y_pred = regr.predict(X_test)\n","    # Print the defined metrics above for each classifier\n","    print_score(y_test, y_pred)"],"metadata":{"id":"cFbrsc5dl7au"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IV.2. Hyperparameters tuning and model optimization"],"metadata":{"id":"0v8w44J8oIGL"}},{"cell_type":"markdown","source":["### IV.2.1. Ridge regression"],"metadata":{"id":"qsZU5ayyoMxe"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define hyperparameters\n","alphas = np.logspace(-5, 5, 50).tolist()\n","\n","tuned_parameters = {\"alpha\": alphas}\n","\n","# GridSearch\n","ridge_cv = GridSearchCV(Ridge(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n","\n","# fit the GridSearch on train set\n","ridge_cv.fit(X_train, y_train)\n","\n","# print best params and the corresponding R²\n","print(f\"Best hyperparameters: {ridge_cv.best_params_}\")\n","print(f\"Best R² (train): {ridge_cv.best_score_}\")"],"metadata":{"id":"wDmi3Kwzocpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ridge Regressor with the best hyperparameters\n","ridge_mod = Ridge(alpha=ridge_cv.best_params_[\"alpha\"])\n","\n","# Fit the model on train set\n","ridge_mod.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = ridge_mod.predict(X_test)\n","\n","print(f\"- {ridge_mod.__class__.__name__}\")\n","print(f\"R²: {rsqr_score(y_test, y_pred)}\")\n","print(f\"RMSE: {rmse_score(y_test, y_pred)}\")"],"metadata":{"id":"D2wJJ6tNiKXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model results into lists\n","model_list = []\n","r2_list = []\n","rmse_list = []\n","\n","model_list.append(ridge_mod.__class__.__name__)\n","r2_list.append(round(rsqr_score(y_test, y_pred), 4))\n","rmse_list.append(round(rmse_score(y_test, y_pred), 4))"],"metadata":{"id":"CF3W0km6iat3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot Actual vs. Predicted house prices\n","actual_price = np.exp(y_test[\"SalePriceLog\"])\n","predicted_price = np.exp(y_pred)\n","\n","plt.figure()\n","plt.title(\"Actual vs. Predicted house prices\\n (Ridge)\", fontsize=20)\n","plt.scatter(actual_price, predicted_price,\n","            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\n","plt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\n","plt.xlim(0, 800000)\n","plt.ylim(0, 800000)\n","plt.xlabel(\"\\nActual Price\", fontsize=16)\n","plt.ylabel(\"Predicted Price\\n\", fontsize=16)\n","plt.show()"],"metadata":{"id":"CEqDgS_eihJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IV.2.2. Lasso regression"],"metadata":{"id":"Oshbo8iLol3i"}},{"cell_type":"code","source":[],"metadata":{"id":"932niy46owUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Dieb-KKgizst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rnXHsuqrizfJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IV.2.3. XGBoost regression"],"metadata":{"id":"phdHA9_5o34U"}},{"cell_type":"code","source":["# Define hyperparameters\n","tuned_parameters = {\"max_depth\": [3],\n","                    \"colsample_bytree\": [0.3, 0.7],\n","                    \"learning_rate\": [0.01, 0.05, 0.1],\n","                    \"n_estimators\": [100, 500]}\n","\n","# GridSearch\n","xgbr_cv = GridSearchCV(estimator=XGBRegressor(),\n","                       param_grid=tuned_parameters,\n","                       cv=5,\n","                       n_jobs=-1,\n","                       verbose=1)\n","\n","# fit the GridSearch on train set\n","xgbr_cv.fit(X_train, y_train)\n","\n","# print best params and the corresponding R²\n","print(f\"Best hyperparameters: {xgbr_cv.best_params_}\\n\")\n","print(f\"Best R²: {xgbr_cv.best_score_}\")"],"metadata":{"id":"5qxUqSjVixwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# XGB Regressor with the best hyperparameters\n","xgbr_mod = XGBRegressor(\n","    seed=20,\n","    colsample_bytree=xgbr_cv.best_params_[\"colsample_bytree\"],\n","    learning_rate=xgbr_cv.best_params_[\"learning_rate\"],\n","    max_depth=xgbr_cv.best_params_[\"max_depth\"],\n","    n_estimators=xgbr_cv.best_params_[\"n_estimators\"]\n",")\n","\n","# Fit the model on train set\n","xgbr_mod.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = xgbr_mod.predict(X_test)\n","\n","print(f\"- {xgbr_mod.__class__.__name__}\")\n","print(f\"R²: {rsqr_score(y_test, y_pred)}\")\n","print(f\"RMSE: {rmse_score(y_test, y_pred)}\")"],"metadata":{"id":"gp_Wjj8vi0dO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model results into lists\n","model_list.append(xgbr_mod.__class__.__name__)\n","r2_list.append(round(rsqr_score(y_test, y_pred), 4))\n","rmse_list.append(round(rmse_score(y_test, y_pred), 4))"],"metadata":{"id":"kFoEHVqki0as"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot Actual vs. Predicted house prices\n","actual_price = np.exp(y_test[\"SalePriceLog\"])\n","predicted_price = np.exp(y_pred)\n","\n","plt.figure()\n","plt.title(\"Actual vs. Predicted house prices\\n (XGBoost)\", fontsize=20)\n","plt.scatter(actual_price, predicted_price,\n","            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\n","plt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\n","plt.xlim(0, 800000)\n","plt.ylim(0, 800000)\n","plt.xlabel(\"\\nActual Price\", fontsize=16)\n","plt.ylabel(\"Predicted Price\\n\", fontsize=16)\n","plt.show()"],"metadata":{"id":"EoKszJdei0X4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IV.2.4. LightGBM Regression"],"metadata":{"id":"CPdyDjAZpMUi"}},{"cell_type":"code","source":[],"metadata":{"id":"GB3PrLzMpIAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tZk4TUahpO-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bFxfdJmOpXNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LxY2aM0Ppg8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IV.3. Choosing the best model"],"metadata":{"id":"JTkWp3w4pnyS"}},{"cell_type":"code","source":[],"metadata":{"id":"s6aSHskApjVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#V. Conclusion"],"metadata":{"id":"3M2Yfu-k_qGs"}},{"cell_type":"code","source":[],"metadata":{"id":"LhrookYg_phr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hhEQ_b0vpqUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vpZF1JhCqJvq"},"execution_count":null,"outputs":[]}]}